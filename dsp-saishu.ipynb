{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd10f71",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re\n",
    "\n",
    "# WebサイトのトップページURL\n",
    "BASE_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "# 同一ドメインのURLを格納するキュー（未探索）\n",
    "queue = {BASE_URL}\n",
    "# 既に探索済みのURL\n",
    "visited = set()\n",
    "# サイトマップを格納する辞書\n",
    "sitemap = {}\n",
    "# Webサイトへの負荷軽減のための待機時間（秒）\n",
    "WAIT_TIME = 1.0\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    URLが同一ドメインであり、除外する拡張子でないかを確認する\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    # 同一ドメインの確認\n",
    "    if parsed_url.netloc != urlparse(BASE_URL).netloc:\n",
    "        return False\n",
    "    # ファイル拡張子の除外 (.pdf, .jpeg, .png)\n",
    "    EXCLUDE_EXTENSIONS = ['.pdf', '.jpeg', '.png', '.jpg', '.gif']\n",
    "    if any(url.lower().endswith(ext) for ext in EXCLUDE_EXTENSIONS):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def scrape_sitemap():\n",
    "    \"\"\"\n",
    "    Webサイトを巡回し、サイトマップを抽出するメイン関数\n",
    "    \"\"\"\n",
    "    print(f\"--- サイトマップ抽出開始: {BASE_URL} ---\")\n",
    "    \n",
    "    while queue and len(sitemap) < 500: # 無限ループ回避のため上限を設定（任意）\n",
    "        current_url = queue.pop()\n",
    "        \n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"探索中: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            # 負荷軽減のための待機\n",
    "            time.sleep(WAIT_TIME)\n",
    "            \n",
    "            # Webサイトにアクセス\n",
    "            response = requests.get(current_url, timeout=10)\n",
    "            response.raise_for_status() # 200以外の場合は例外発生\n",
    "            \n",
    "            # エンコーディングの自動検出（文字化け対策）\n",
    "            response.encoding = response.apparent_encoding\n",
    "            html_content = response.text\n",
    "            \n",
    "            # 探索済みに登録\n",
    "            visited.add(current_url)\n",
    "            \n",
    "            # BeautifulSoupでHTMLを解析\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # <title>タグの内容を取得し、辞書に格納\n",
    "            title_tag = soup.find('title')\n",
    "            page_title = title_tag.string.strip() if title_tag and title_tag.string else \"タイトルなし\"\n",
    "            sitemap[current_url] = page_title\n",
    "            \n",
    "            # リンクを抽出\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                # 完全なURLに変換（相対パスに対応）\n",
    "                link_url = urljoin(current_url, a_tag['href'])\n",
    "                \n",
    "                # HTMLコメント内のリンクを除外 (BeautifulSoupはコメント内の要素を直接解析しないため、テキストからチェック)\n",
    "                # ただし、完全なチェックは複雑なため、ここではa_tagが直接取得できたことを優先し、簡易的なチェックとする\n",
    "                # コメントアウトされた要素を特定するには、HTMLの生データを扱う必要がある\n",
    "                \n",
    "                # 有効なリンク（同一ドメイン、除外拡張子でない）か確認\n",
    "                if is_valid(link_url) and link_url not in visited:\n",
    "                    queue.add(link_url)\n",
    "                    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"エラー発生: {current_url} - {e}\")\n",
    "            visited.add(current_url) # エラーが発生したURLも再探索しないように登録\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"その他のエラー: {current_url} - {e}\")\n",
    "            visited.add(current_url)\n",
    "\n",
    "    print(\"\\n--- サイトマップ抽出完了 ---\")\n",
    "    \n",
    "    # 抽出結果の表示\n",
    "    print(\"\\n--- 抽出結果 (URL: <title>) ---\")\n",
    "    for url, title in sitemap.items():\n",
    "        print(f\"'{url}': '{title}'\")\n",
    "\n",
    "    print(f\"\\n合計ページ数: {len(sitemap)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_sitemap()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
